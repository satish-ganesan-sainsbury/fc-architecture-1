# Dataservices Platform - Integration Architecture Principles

1. ***Universality***: Data Services Platform for workstreams should be universal. No workstream to undertake any consumer specific Data Services. It is the responsibility of the consumers to transform the data to their specific requirements

2. ***Strictly one-way Depenency***: Data Services Platform is to be 100% slave of Data Management Platform. Data Services Platform:
Can be completely destroyed and rebuilt from data sources
   * Should not generate data
   * Should not amend any data
   * Should not own any business logic

3. ***Align responsibilities by upstream and downstream pipelines***: Data Services Platform would be segrageted to two pipelines.
   * **Data & Events -> Raw data pipeline(upstream)**: The raw data sourced from the data sources would be maintained in their original format in the raw pipeline. The *Data & Events team* would be responsbile for landing the data in the Kafka topic from the data source
   * **Workstreams -> Consumable data pipeline (downstream)**: Each *workstream team* would be responsible for transforming the raw data into consumable data

4. ***Dual refresh modes***: Data Services Platform should provide for two kinds of refreshe modes:
   * ***Off-set based delta streams***: This would allow consumers to receive only delta changes since the last refresh
   * ***Universal __latest__ data set of unique records***: A materialized view of the latest state of the entire universe of uniqueq records in the topic starting from the recent refresh


5. ***Intelligent stitching for projection***: Although the Data Services Platform would present a universal interface to clients, the information should be organized and classified into groups of attributes. Users should be able to subscribe or access a set of these groupings of their choice along with the core information. 

 
6. ***End-to-end traceability***: Events would flow from one topic to another topic as events flow from data source to clients. It is necessary to have an unique identifier that traces uniquely each event end-to-end

7. ***Mediate unique ids***: Each event typically would be associated with a particular business entity. This business entity would have its own unique id as generated by its current soure system. This unique id might not fit into newer MDM systems into which these events would be migrated. The MDM systems also can not be relied to generate the ids during the migration process as there might be clash with existing ids. Hence, a thid party service is required to mediate and generate unique ids

8. ***Enable choices in Data Consumption***: The platform should provide for flexibility to the consumers for consuming data. At the minimum the consumers shoul be offered:
   * Push based data access using a streaming platform
   * Pull(query) based data access using a API

9. ***KISS for Pull based APIs***: The pull based access should follow the Keep It Simple principle and should follow the universaility principle:
   * Projection choices to be limited to stitching together groups of attributes
   * Avoid dynamic querying that allowsend-users free-form querying against the queryable dataset
